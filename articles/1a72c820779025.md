---
title: "Ryzen 7735HSなThinkPadをollama serverにする"
emoji: "💭"
type: "tech" # tech: 技術記事 / idea: アイデア
topics: ["windows", "ai", "ollama", "ローカルllm"]
published: true
---

## やれるようになったこと

- thinkpad (32GB RAM)でollama動作
- macbookからそれにアクセスしてai使用

## ollamaのインストール

https://ollama.com

からwindows向けにダウンロード&インストール。インストール後に、ターミナルでモデルをダウンロードする。今回入れたモデルは以下の4つ。RAMは32あるのでちょっと大きめのモデルも入れました。

- gemma3:4b
- gemma3:12b
- deepseek-r1:8b
- nomic-embed-text:latest

```powershell
ollama pull gemma3:4b
ollama pull gemma3:12b
ollama pull deepseek-r1:8b
ollama pull nomic-embed-text
```

## portとかの解放

このままだとwindowsがfirewallで弾いてしまうので、ポートを解放します。(linuxならufwで一発なのに)

もし、defenderではなく他のセキュリティを使ってる場合は、そのソフトの設定でポートを解放してください。

windows defenderの左のペインからファイアーウォールの項目、下の方にある詳細設定を開く。右上あたりのルールの追加を押して、11434[^1]を許可します。

あと、なんかollama側でもネットワーク越しのリクエストを受けるため(?)に設定が必要らしく以下です。

```powershell
sudo setx OLLAMA_HOST "0.0.0.0:11434" /M
```

:::details pingについて
ちなみにおそらくデフォルトではpingも返さないので、先ほどのファイアーウォール設定で`File and Printer Sharing (Echo Request - ICMPv4-In)`ファイルとプリンターの共有の(おそらくprivate)を許可にするとpingが通るはず
:::

## macから接続可能か

まずwindowsのipアドレスを調べます。今回は1.7なので以下のようにします。これで`Ollama is running%`と返ってくれば成功です。ダメなら、windowsの再起動など。あんま詳しく知らないので...

```zsh
$ curl http://192.168.1.7:11434
Ollama is running%
```


[^1]: これはollamaのデフォルトポートですが、`llama`を数字表すと11AMA(AとMをそれぞれ横に)になるからだそうです。

## editorでの設定

### vscodeの場合

https://marketplace.visualstudio.com/items?itemName=Continue.continue

continueという拡張機能を使って、ollamaに接続します。しかしそのままではローカルのollamaに繋がってしまうので、apiBaseに先ほどのurlとポートを入れます。

:::message
以前、設定は`~/.continue/config.json`だったのですが、現在はyamlになって`~/.continue/config.yaml`になっているそう。私はこの機会に移行しました。
:::

:::details yaml ver. config
```yaml
name: Server Assistant
version: 1.0.0
schema: v1
context:
  - provider: code
  - provider: docs
  - provider: diff
  - provider: terminal
  - provider: problems
  - provider: folder
  - provider: codebase

models:
  - model: gemma3:12b
    provider: ollama
    name: gemma3_12b
    apiBase: http://192.168.1.7:11434
  - model: deepseek-r1:8b
    provider: ollama
    name: deepseek_r1_8b
    apiBase: http://192.168.1.7:11434

contextProviders:
  - name: code
    params: {}
  - name: docs
    params: {}
  - name: diff
    params: {}
  - name: terminal
    params: {}
  - name: problems
    params: {}
  - name: folder
    params: {}
  - name: codebase
    params: {}

slashCommands:
  - name: share
    description: Export the current chat session to markdown
  - name: cmd
    description: Generate a shell command
  - name: commit
    description: Generate a git commit message

embeddingsProvider:
  provider: ollama
  model: nomic-embed-text

reranker:
  name: free-trial

# apiBase: http://localhost:11434/v1

customCommands:
  - name: test
    prompt: "{{{ input }}}\n\nWrite a comprehensive set of unit tests for the selected code. It should setup, run tests that check for correctness including important edge cases, and teardown. Ensure that the tests are complete and sophisticated. Give the tests just as chat output, don't edit any file."
    description: "This is an example custom command. Open config.json to edit it and create more"
  - name: doc
    description: "write a doc for code."
    prompt: "write a only doc comment. don't write a function code . i want to paste it as it is."

tabAutocompleteOptions:
  disable: false
```
:::

### zedの場合

zedは`~/.config/zed/config.json`に`language_models`という項目を入れれば動きます。最初、`supports_tools`という項目を設定していたのですが、エラーになり、chatGPT曰くgemmaはツールに対応してないからエラーになったとのこと。

:::details ~/.config/zed/config.json
```json
// Zed settings
//
// For information on how to configure Zed, see the Zed
// documentation: https://zed.dev/docs/configuring-zed
//
// To see all of Zed's default settings without changing your
// custom settings, run `zed: open default settings` from the
// command palette (cmd-shift-p / ctrl-shift-p)
{
  // ~~~省略~~~
  "language_models": {
    "ollama": {
      "api_url": "http://192.168.1.7:11434",
      "available_models": [
        {
          "name": "gemma3:12b",
          "max_tokens": 32768,
          "display_name": "gemma3 12b"
        },
        {
          "name": "gemma3:4b",
          "max_tokens": 32768,
          "display_name": "gemma3 4b"
        }
      ]
    }
  }
}
```
:::

## まとめ

これで16GB macbookを苦しめずにローカルllmができます。まあ家から出れば使えませんが。

あと、構築が楽しかっただけで普段からollama使ってるわけでないことは秘密。
