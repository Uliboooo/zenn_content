---
title: "how to impl progress spinner on rust cli"
emoji: "߷"
type: "tech" # tech: 技術記事 / idea: アイデア
topics: ["rust"]
published: false
---



pub fn call_llm<T: AsRef<str>>(
    pmt: T,
    provider: T,
    model: T,
    api_key: Option<String>,
    temperature: Option<f32>,
    max_tokens: Option<u32>,
) -> Result<String, LlmError> {
    let model = model.as_ref().to_string();
    let pmt = pmt.as_ref().to_string();
    let rt = Runtime::new().unwrap();

    let api_key = match api_key {
        Some(v) => v,
        None => {
            if provider.as_ref().to_lowercase() != "ollama" {
                return Err(LlmError::NotFoundAPIKey);
            } else {
                String::new()
            }
        }
    };
    match provider.as_ref().to_lowercase().as_str() {
        "ollama" => rt.block_on(ollama(pmt, model)),
        "anthropic" => rt.block_on(anthropic(api_key, model, pmt, temperature, max_tokens)),
        "deepseek" => rt.block_on(deep_seek(api_key, model, pmt, temperature, max_tokens)),
        "gemini" => rt.block_on(gemini(api_key, model, pmt, temperature, max_tokens)),
        "openai" => rt.block_on(openai(api_key, model, pmt, temperature, max_tokens)),
        _ => Err(LlmError::UndefinedProvider),
    }
}

async fn ollama(pmt: String, model: String) -> Result<String, LlmError> {
    let ollama = Ollama::default();

    let res = ollama.generate(GenerationRequest::new(model, pmt)).await;
    match res {
        Ok(v) => Ok(v.response.to_string()),
        Err(e) => Err(LlmError::Ollama(e)),
    }
}
